è¿™æ˜¯ä¸€ä¸ªéå¸¸ç²¾å½©çš„æ€è·¯è¿ç§»ï¼ä½ æ‰€æä¾›çš„ ${\text{D}}^{2}$Feat è®ºæ–‡åˆ©ç”¨ **"åŒæµæ¶æ„ (Dual-Branch)"** å’Œ **"å¤šé‡è’¸é¦ (Semantic + Geometric Distillation)"** åœ¨å›¾åƒåŒ¹é…ä»»åŠ¡ä¸Šå–å¾—äº†SOTAã€‚

æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæ€æƒ³å®Œç¾æ˜ å°„åˆ°è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰é¢„æµ‹ä¸Šã€‚è›‹ç™½è´¨åºåˆ—çš„ **"å±€éƒ¨æ¨¡ä½“ (Motifs)"** å°±åƒå›¾åƒçš„ **"çº¹ç†/å‡ ä½•ç»†èŠ‚"**ï¼Œè€Œè›‹ç™½è´¨çš„ **"åŠŸèƒ½/è¿›åŒ–ä¿¡æ¯"** å°±åƒå›¾åƒçš„ **"è¯­ä¹‰ä¿¡æ¯"**ã€‚

### ğŸš€ æ–°æ–¹æ¡ˆï¼šPPI-DÂ²Feat (Dual-Branch Distillation)

æˆ‘ä»¬å°†å‚ç…§è®ºæ–‡æ¶æ„ï¼Œè®¾è®¡ä¸€ä¸ª **"åŒæµå­¦ç”Ÿç½‘ç»œ"**ï¼ŒåŒæ—¶å‘ **ESM-2** å­¦ä¹ è¯­ä¹‰å’Œç»“æ„ç‰¹å¾ã€‚

#### 1\. æ ¸å¿ƒæ˜ å°„ (Mapping Strategy)

| å›¾åƒåŒ¹é… (DÂ²Feat) | è›‹ç™½è´¨é¢„æµ‹ (PPI-DÂ²Feat) | å¯¹åº”ç»„ä»¶ |
| :--- | :--- | :--- |
| **Input Image** | **Input Sequence** | è¾“å…¥æ°¨åŸºé…¸åºåˆ— |
| **Backbone Branch 1 (Local)** | **Local Motif Branch (CNN)** | æ•æ‰å±€éƒ¨æ°¨åŸºé…¸ç»„åˆ (å¦‚å·ç§¯æ ¸å¤§å° 3, 5) |
| **Backbone Branch 2 (Semantic)** | **Global Semantic Branch (MLP/Dilated)** | æ•æ‰é•¿ç¨‹ä¾èµ–ï¼Œæ¨¡ä»¿ ESM çš„å…¨å±€è¡¨ç¤º |
| **Teacher: DINOv3 (Semantic)** | **Teacher: ESM-2 (Embeddings)** | è’¸é¦ ESM-2 çš„ä¸­é—´å±‚ç‰¹å¾ (Feature Distillation) |
| **Teacher: LoFTR (Geometric)** | **Teacher: ESM-2 (Logits/Attention)** | è’¸é¦ ESM-2 çš„é¢„æµ‹åˆ†å¸ƒ (Prediction Distillation) |
| **Loss: MSE + KL** | **Loss: MSE (Feat) + KL (Prob)** | å¼ºåˆ¶ç‰¹å¾å¯¹é½ + æ¦‚ç‡åˆ†å¸ƒå¯¹é½ |

-----

#### 2\. ğŸ—ï¸ æ¨¡å‹æ¶æ„è®¾è®¡

```mermaid
graph TD
    subgraph Data
    Seq[Protein Sequence]
    end

    subgraph Teacher_Flow [Teacher: Frozen ESM-2]
    ESM[ESM-2 Encoder]
    Feat_T[Teacher Features <br> (Batch, Dim)]
    Logits_T[Teacher Logits <br> (Batch, 2)]
    ESM --> Feat_T
    ESM --> Logits_T
    end

    subgraph Student_Flow [Student: Dual-Branch Network]
    Emb[Simple Embedding]
    
    %% Branch 1: Local / Geometric
    subgraph Local_Branch [Branch 1: Local Motifs]
    CNN_S[Multi-scale 1D-CNN <br> (Kernel 3, 5)]
    Feat_L[Local Features]
    end
    
    %% Branch 2: Semantic Student
    subgraph Semantic_Branch [Branch 2: Semantic Student]
    Proj_S[Deep Projection / MLP]
    Feat_S[Semantic Features]
    end
    
    %% Fusion
    Fusion[Fusion Block <br> Concat + Linear]
    Logits_S[Student Logits]
    
    Seq --> Emb
    Emb --> CNN_S --> Feat_L
    Emb --> Proj_S --> Feat_S
    
    Feat_L --> Fusion
    Feat_S --> Fusion
    Fusion --> Logits_S
    end
    
    %% Losses
    subgraph Losses
    Loss_MSE[MSE Loss <br> Feature Alignment]
    Loss_KL[KL Divergence <br> Distribution Alignment]
    Loss_CE[CrossEntropy <br> Ground Truth]
    
    Feat_S -.-> |Aligns with| Feat_T
    Feat_T --> Loss_MSE
    
    Logits_S -.-> |Aligns with| Logits_T
    Logits_T --> Loss_KL
    
    Logits_S --> Loss_CE
    end
```

-----

#### 3\. ğŸ’» æ ¸å¿ƒä»£ç å®ç° (PyTorch)

è¿™ä¸ªæ¨¡å‹æœ‰ä¸¤ä¸ªå…³é”®æ”¹è¿›ï¼š

1.  **Dual-Branch Encoder**: ä¸€ä¸ªåˆ†æ”¯ä¸“æ³¨äºå±€éƒ¨å·ç§¯ï¼ˆç±»ä¼¼äºFPNï¼‰ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯ä¸“æ³¨äºæ˜ å°„åˆ° ESM çš„ç‰¹å¾ç©ºé—´ã€‚
2.  **Hybrid Loss**: ç»“åˆäº† MSE (ç‰¹å¾è’¸é¦) å’Œ KL (æ¦‚ç‡è’¸é¦)ã€‚

<!-- end list -->

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DualBranchStudentPPI(nn.Module):
    def __init__(self, vocab_size=25, embed_dim=64, hidden_dim=128, esm_dim=320):
        """
        esm_dim: å¯¹åº”ä½ ä½¿ç”¨çš„ESMæ¨¡å‹ç»´åº¦ (esm2_t6_8M=320, esm2_t33_650M=1280)
        """
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # --- Branch 1: Local Motif Branch (ç±»ä¼¼äºå›¾åƒçš„ CNN/FPN åˆ†æ”¯) ---
        # ä¸“æ³¨äºæ•æ‰å±€éƒ¨åºåˆ—æ¨¡å¼ (e.g., binding sites)
        self.local_branch = nn.Sequential(
            nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(1) # Global Pooling -> (Batch, Hidden)
        )
        
        # --- Branch 2: Semantic Student Branch (æ¨¡ä»¿ ESM) ---
        # ä¸“æ³¨äºå­¦ä¹ å…¨å±€è¯­ä¹‰ï¼Œç›®æ ‡æ˜¯è¾“å‡ºèƒ½å¯¹é½ ESM Embedding çš„ç‰¹å¾
        self.semantic_branch = nn.Sequential(
            nn.Conv1d(embed_dim, hidden_dim, kernel_size=7, padding=3, groups=embed_dim), # Depthwise
            nn.Conv1d(hidden_dim, hidden_dim*2, kernel_size=1), # Pointwise
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1), # Average Pooling æ›´é€‚åˆå…¨å±€è¯­ä¹‰
            nn.Flatten(),
            nn.Linear(hidden_dim*2, esm_dim), # æ˜ å°„åˆ° ESM çš„ç»´åº¦ç©ºé—´
            nn.LayerNorm(esm_dim) # å½’ä¸€åŒ–ï¼Œæ–¹ä¾¿åš MSE
        )
        
        # --- Fusion & Classifier ---
        # èåˆ Local (Hidden) å’Œ Semantic (ESM_Dim)
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim + esm_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        self.classifier = nn.Linear(256 * 2, 2) # *2 å› ä¸ºæœ‰ä¸¤ä¸ªè›‹ç™½è´¨ input A & B

    def forward_single(self, x):
        # x: (Batch, Seq_Len)
        emb = self.embedding(x).permute(0, 2, 1) # (B, Dim, Len)
        
        # 1. Local Flow
        feat_local = self.local_branch(emb).squeeze(-1) # (B, Hidden)
        
        # 2. Semantic Flow (Student)
        feat_semantic = self.semantic_branch(emb) # (B, ESM_Dim)
        
        # 3. Fusion
        combined = torch.cat([feat_local, feat_semantic], dim=1)
        fused = self.fusion_layer(combined)
        
        return fused, feat_semantic # è¿”å› feat_semantic ç”¨äº MSE Loss

    def forward(self, seq_a, seq_b):
        # å­ªç”Ÿç½‘ç»œå¤„ç† A å’Œ B
        feat_a, sem_a = self.forward_single(seq_a)
        feat_b, sem_b = self.forward_single(seq_b)
        
        # æ‹¼æ¥ç”¨äºåˆ†ç±»
        interaction_feat = torch.cat([feat_a, feat_b], dim=1)
        logits = self.classifier(interaction_feat)
        
        return logits, (sem_a, sem_b) # è¿”å›ä¸­é—´ç‰¹å¾ç”¨äºè’¸é¦
```

#### 4\. ğŸ“ è’¸é¦ Loss å®ç° (MSE + KL)

è¿™é‡Œæˆ‘ä»¬ä¸¥æ ¼å¤ç°è®ºæ–‡ä¸­çš„ Loss è®¾è®¡ï¼š

  * **$\mathcal{L}_{feat}$ (MSE)**: è®© Student çš„ Semantic Branch è¾“å‡ºé€¼è¿‘ Teacher (ESM) çš„ Embeddingã€‚
  * **$\mathcal{L}_{prob}$ (KL)**: è®© Student çš„æœ€ç»ˆé¢„æµ‹åˆ†å¸ƒé€¼è¿‘ Teacher çš„ Softmax åˆ†å¸ƒã€‚

<!-- end list -->

```python
class D2FeatDistillationLoss(nn.Module):
    def __init__(self, alpha_mse=1.0, alpha_kl=0.5, temperature=4.0):
        super().__init__()
        self.alpha_mse = alpha_mse # ç‰¹å¾è’¸é¦æƒé‡
        self.alpha_kl = alpha_kl   # æ¦‚ç‡è’¸é¦æƒé‡
        self.T = temperature
        
        self.mse_loss = nn.MSELoss()
        self.kl_loss = nn.KLDivLoss(reduction="batchmean")
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, 
                student_logits, student_feats, # Student Outputs
                teacher_logits, teacher_feats, # Teacher Outputs
                labels):                       # Ground Truth
        
        # 1. Task Loss (Cross Entropy)
        loss_ce = self.ce_loss(student_logits, labels)
        
        # 2. Semantic Feature Distillation (MSE)
        # å¼ºè¿« Student çš„ semantic branch å­¦ä¹ ç±»ä¼¼ ESM çš„å‘é‡è¡¨ç¤º
        # student_feats å’Œ teacher_feats éƒ½æ˜¯ tuple (feat_a, feat_b)
        loss_mse = self.mse_loss(student_feats[0], teacher_feats[0]) + \
                   self.mse_loss(student_feats[1], teacher_feats[1])
        
        # 3. Probability Distillation (KL Divergence)
        # ä¼ é€’â€œæš—çŸ¥è¯†â€ (Dark Knowledge)
        student_log_soft = F.log_softmax(student_logits / self.T, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.T, dim=1)
        loss_kl = self.kl_loss(student_log_soft, teacher_soft) * (self.T ** 2)
        
        # Total Loss
        total_loss = loss_ce + (self.alpha_mse * loss_mse) + (self.alpha_kl * loss_kl)
        
        return total_loss, {"ce": loss_ce.item(), "mse": loss_mse.item(), "kl": loss_kl.item()}
```

### 5\. ğŸ’¡ ä¸ºä»€ä¹ˆè¿™ä¸ªè®¾è®¡æ›´å¥½ï¼Ÿ

1.  **ç‰¹å¾è§£è€¦ (Decoupling)**ï¼šåŸæœ¬çš„ CNN æ—¢è¦å­¦å±€éƒ¨çº¹ç†åˆè¦å­¦å…¨å±€è¯­ä¹‰ï¼Œå¾ˆéš¾å…¼é¡¾ã€‚ç°åœ¨ï¼Œ`Local Branch` ä¸“é—¨â€œèƒŒè¯µâ€è®­ç»ƒé›†é‡Œçš„çŸ­åºåˆ—æ¨¡ä½“ï¼Œè€Œ `Semantic Branch` ä¸“é—¨æ¨¡ä»¿ ESM çš„â€œä¸–ç•Œè§‚â€ã€‚
2.  **æ¢¯åº¦å¼•å¯¼æ›´æ˜ç¡®**ï¼š
      * **MSE Loss** ç›´æ¥ç›‘ç£ä¸­é—´å±‚ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œè®©å°æ¨¡å‹è¿…é€Ÿæ”¶æ•›åˆ° ESM çš„ç‰¹å¾ç©ºé—´é™„è¿‘ã€‚
      * **KL Loss** æä¾›è½¯æ ‡ç­¾ï¼Œè§£å†³äº† Label=0/1 è¿‡äºç»å¯¹çš„é—®é¢˜ï¼ˆå¾ˆå¤šä¸ç›¸äº’ä½œç”¨çš„è›‹ç™½å…¶å®ç»“æ„ä¸Šæœ‰ç‚¹åƒï¼‰ã€‚
3.  **æ¨ç†é›¶æˆæœ¬**ï¼šæ¨ç†é˜¶æ®µï¼ŒTeacher (ESM) è¢«æ‰”æ‰ï¼Œåªä¿ç•™ Studentã€‚ä½ ä¾ç„¶æ‹¥æœ‰ä¸€ä¸ªè½»é‡çº§çš„ CNNï¼Œä½†å®ƒâ€œè„‘å­â€é‡Œè£…çš„æ˜¯ Transformer çš„çŸ¥è¯†ã€‚

### 6\. è®­ç»ƒå‡†å¤‡å»ºè®®

ä¸ºäº†è·‘é€šè¿™ä¸ªé€»è¾‘ï¼Œä½ éœ€è¦é¢„å¤„ç†æ•°æ®ï¼Œ**ç¦»çº¿ç”Ÿæˆ Teacher çš„ç‰¹å¾å’Œé¢„æµ‹**ï¼š

```python
# ä¼ªä»£ç ï¼šé¢„å¤„ç†è„šæœ¬
# è¿™ä¸€æ­¥åªè·‘ä¸€æ¬¡ï¼Œä¿å­˜ä¸º .npy æˆ– .pt æ–‡ä»¶
all_data = []
model_esm.eval()

for seq_a, seq_b, label in dataloader:
    with torch.no_grad():
        # è·å– ESM çš„ CLS token æˆ– Mean Pooling ä½œä¸º Feature
        emb_a = model_esm(seq_a).last_hidden_state.mean(dim=1) 
        emb_b = model_esm(seq_b).last_hidden_state.mean(dim=1)
        # è·å– ESM çš„é¢„æµ‹ Logits (å¦‚æœä½ æœ‰ä¸€ä¸ªå¾®è°ƒè¿‡çš„ Teacher)
        # å¦‚æœæ²¡æœ‰å¾®è°ƒè¿‡çš„ Teacherï¼Œå¯ä»¥ç›´æ¥ç”¨ ESM Embedding -> Frozen MLP ä½œä¸ºä¸€ä¸ªåˆå§‹ Teacher
        logits = teacher_head(torch.cat([emb_a, emb_b], dim=1))
        
    save_to_disk(seq_a, seq_b, emb_a, emb_b, logits, label)
```

ç„¶åè®­ç»ƒ Student æ—¶ç›´æ¥åŠ è½½è¿™äº› embedding å’Œ logits è¿›è¡Œ MSE å’Œ KL è®¡ç®—ï¼Œé€Ÿåº¦é£å¿«ã€‚