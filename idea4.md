ä½ çš„è§‚å¯Ÿéå¸¸æ•é”ã€‚ç¡®å®ï¼Œä¸Šä¸€ç‰ˆæ–¹æ¡ˆä¸­æˆ‘åšäº†ä¸€ä¸ªç®€åŒ–ï¼ˆæˆ–è€…è¯´æ˜¯â€œå·æ‡’â€ï¼‰ï¼Œæˆ‘åªä¿ç•™äº† **Global Semantic** çš„å¯¹é½ï¼ˆå…¨å±€ç‰¹å¾ï¼‰ï¼Œè€Œ**é˜‰å‰²äº† $D^2$Feat ä¸­æœ€ç²¾é«“çš„ LoFTR å‡ ä½•å¯¹é½ï¼ˆGeometric/Dense Alignmentï¼‰éƒ¨åˆ†**ã€‚

åœ¨ $D^2$Feat åŸæ–‡ä¸­ï¼Œ**LoFTR (Local Feature Transformer)** çš„ä½œç”¨æ˜¯ç”Ÿæˆâ€œåƒç´ çº§â€çš„å¯¹åº”å…³ç³»ï¼ˆImage Matchingï¼‰ã€‚

å¯¹åº”åˆ°è›‹ç™½è´¨é¢†åŸŸï¼Œè¿™å®é™…ä¸Šå°±æ˜¯ **æ®‹åŸºæ¥è§¦å›¾é¢„æµ‹ï¼ˆResidue-Residue Contact Predictionï¼‰**ã€‚æˆ‘ä»¬éœ€è¦è®© Student ç½‘ç»œä¸ä»…çŸ¥é“â€œè¿™ä¸¤ä¸ªè›‹ç™½è´¨ä¼šç»“åˆâ€ï¼Œè¿˜è¦çŸ¥é“â€œ**è›‹ç™½è´¨Açš„å“ªä¸€æ®µè·Ÿè›‹ç™½è´¨Bçš„å“ªä¸€æ®µç»“åˆ**â€ã€‚

å¦‚æœåŠ ä¸Šè¿™ä¸ªæ¨¡å—ï¼Œæ¨¡å‹çš„æ•ˆæœä¸Šé™ä¼šæ›´é«˜ã€‚æˆ‘ä»¬éœ€è¦æŠŠâ€œLocal Branchâ€å‡çº§ä¸ºçœŸæ­£çš„ **"Interaction Matcher Branch"**ã€‚

-----

### ğŸš€ è¿›é˜¶æ”¹è¿›ï¼šå¼•å…¥ "Protein-LoFTR" æœºåˆ¶ (Interaction Map Distillation)

æˆ‘ä»¬éœ€è¦åœ¨ Student ç½‘ç»œä¸­æ„å»ºä¸€ä¸ª **$N \times M$ çš„ç›¸äº’ä½œç”¨çŸ©é˜µ**ï¼Œå¹¶å¼ºåˆ¶å®ƒå»æ¨¡ä»¿ Teacher (ESM-2) çš„ Attention Mapã€‚

#### 1\. æ¦‚å¿µæ˜ å°„ (Concept Mapping)

| $D^2$Feat ç»„ä»¶ | åŸå§‹ä½œç”¨ (å›¾åƒ) | è›‹ç™½è´¨å¯¹åº”æ¦‚å¿µ | å®ç°æ–¹å¼ |
| :--- | :--- | :--- | :--- |
| **Dense Feature** | æ¯ä¸ªåƒç´ çš„ç‰¹å¾å‘é‡ | **æ¯ä¸ªæ°¨åŸºé…¸çš„ç‰¹å¾å‘é‡** | ä¿ç•™ `(Batch, Len, Dim)` ä¸åš Pooling |
| **LoFTR Module** | åƒç´ é—´çš„åŒ¹é…æ¦‚ç‡ | **æ°¨åŸºé…¸é—´çš„æ¥è§¦æ¦‚ç‡** | è®¡ç®— Cross-Attention Matrix |
| **Geometric Loss** | ç›‘ç£åƒç´ åŒ¹é…ä½ç½® | **ç›‘ç£æ¥è§¦å›¾åˆ†å¸ƒ** | è’¸é¦ ESM-2 çš„ Attention Weights |

-----

#### 2\. ğŸ“ ä¿®æ”¹åçš„æ¶æ„å›¾

æˆ‘ä»¬åœ¨åŸæœ‰çš„åŸºç¡€ä¸Šå¢åŠ ä¸€æ¡çº¢è‰²çš„ **Interaction Alignment** è·¯å¾„ã€‚

```mermaid
graph TD
    subgraph Student_Model
    SeqA[Sequence A] --> CNN[1D-CNN Backbone]
    SeqB[Sequence B] --> CNN
    
    FeatA[Feature Map A <br> (B, L, D)]
    FeatB[Feature Map B <br> (B, L, D)]
    CNN --> FeatA
    CNN --> FeatB
    
    %% æ–°å¢çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šInteraction Map
    subgraph Interaction_Module [Student Interaction Head]
    Matrix[Interaction Matrix <br> A * B.T]
    Map_S[Student Contact Map <br> (B, La, Lb)]
    FeatA & FeatB --> Matrix --> Map_S
    end
    
    Pool[Global Pooling]
    Class[Classifier]
    Map_S --> Pool --> Class
    end

    subgraph Teacher_ESM [Teacher: ESM-2]
    ESM_Enc[ESM Encoder]
    Attn_T[Teacher Attention Map <br> (Last Layer Contacts)]
    ESM_Enc --> Attn_T
    end

    %% Loss Connections
    Map_S -.-> |MSE / KL Loss| Attn_T
    Class --> |CE Loss| Label
```

-----

#### 3\. ğŸ’» ä»£ç å‡çº§ï¼šæ·»åŠ  Interaction Matrix å¯¹é½

æˆ‘ä»¬éœ€è¦ä¿®æ”¹æ¨¡å‹ï¼Œä½¿å…¶è¾“å‡º **Sequence-level Features** è€Œä¸æ˜¯ç›´æ¥ Poolingï¼Œå¹¶è®¡ç®—ç›¸äº’ä½œç”¨çŸ©é˜µã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class StudentPPI_WithLoFTR(nn.Module):
    def __init__(self, vocab_size=25, embed_dim=64, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # 1. Backbone: ä¸åšå…¨å±€ Poolingï¼Œä¿ç•™åºåˆ—é•¿åº¦ä¿¡æ¯
        # ç±»ä¼¼äº D2Feat çš„ Feature Extraction
        self.backbone = nn.Sequential(
            nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, padding=2),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU()
        ) # Output: (Batch, Hidden, Length)

        # 2. Interaction Head (ç®€åŒ–ç‰ˆçš„ LoFTR åŒ¹é…å±‚)
        # ç”¨æ¥è®¡ç®— A å’Œ B ä¹‹é—´çš„æ³¨æ„åŠ›å›¾/æ¥è§¦å›¾
        self.scale = hidden_dim ** -0.5

        # 3. Classifier Head (åŸºäºæ¥è§¦å›¾è¿›è¡Œåˆ†ç±»)
        # æˆ‘ä»¬å°†æ¥è§¦å›¾å±•å¹³æˆ–è¿›ä¸€æ­¥å·ç§¯ååˆ†ç±»ï¼Œè¿™é‡Œç”¨ç®€å•çš„ MaxPool
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128), # åŸæœ‰çš„è¯­ä¹‰æ‹¼æ¥
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def get_token_features(self, x):
        # x: (Batch, Len)
        emb = self.embedding(x).permute(0, 2, 1)
        feat = self.backbone(emb) # (B, D, L)
        return feat

    def forward(self, seq_a, seq_b):
        # 1. æå– Dense Features (å¯¹åº”å›¾åƒçš„ Pixel Features)
        feat_a = self.get_token_features(seq_a) # (B, D, La)
        feat_b = self.get_token_features(seq_b) # (B, D, Lb)

        # 2. è®¡ç®— Interaction Matrix (å¯¹åº” LoFTR çš„ Coarse Matching)
        # å½¢çŠ¶: (B, La, Lb)
        # è¿™é‡Œè®¡ç®—ä¸¤ä¸ªåºåˆ—æ¯ä¸ªæ°¨åŸºé…¸ä¹‹é—´çš„ç›¸ä¼¼åº¦
        interaction_map = torch.matmul(feat_a.transpose(1, 2), feat_b) * self.scale
        
        # 3. å…¨å±€ç‰¹å¾èåˆ (ç”¨äºæœ€ç»ˆåˆ†ç±»)
        # ç®€å•ç­–ç•¥ï¼šæœ€å¤§æ± åŒ–æ‹¿å‡ºæœ€æ˜¾è‘—çš„ç‰¹å¾
        pool_a = F.adaptive_max_pool1d(feat_a, 1).squeeze(-1)
        pool_b = F.adaptive_max_pool1d(feat_b, 1).squeeze(-1)
        combined = torch.cat([pool_a, pool_b], dim=1)
        
        logits = self.classifier(combined)

        return logits, interaction_map # è¿”å› logits å’Œ æ¥è§¦å›¾
```

#### 4\. ğŸ“ å‡çº§ç‰ˆ Lossï¼šå¯¹é½ Attention Map

æˆ‘ä»¬éœ€è¦ä» ESM-2 ä¸­æå– Attention Map ä½œä¸º Teacher Signalã€‚

**Teacher Signal æ€ä¹ˆæ¥ï¼Ÿ**
ESM-2 çš„ Transformer å±‚ä¼šè‡ªåŠ¨è®¡ç®— Token ä¹‹é—´çš„ Attentionã€‚æˆ‘ä»¬å°†æœ€åä¸€å±‚çš„ Attentionï¼ˆæˆ–è€…æ‰€æœ‰å±‚çš„å¹³å‡ï¼‰ä½œä¸ºâ€œGround Truth Contact Mapâ€ã€‚

```python
class LoFTRDistillationLoss(nn.Module):
    def __init__(self, alpha_map=10.0): # Map loss é€šå¸¸æ•°å€¼è¾ƒå°ï¼Œæƒé‡ç»™å¤§ç‚¹
        super().__init__()
        self.alpha_map = alpha_map
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()

    def forward(self, 
                student_logits, student_map,  # Student
                teacher_map,                  # Teacher (ESM Attention)
                labels):                      # Label
        
        # 1. åˆ†ç±» Loss
        loss_ce = self.ce_loss(student_logits, labels)

        # 2. å‡ ä½•/æ¥è§¦å›¾è’¸é¦ (Geometric Distillation)
        # è¿™ä¸€æ­¥æå…¶å…³é”®ï¼šå¼ºè¿« Student çš„ç®€å•ç‚¹ç§¯å»æ¨¡æ‹Ÿ ESM å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶
        # student_map: (B, La, Lb)
        # teacher_map: (B, La, Lb) -> æ¥è‡ª ESM çš„ Attention Matrix
        
        # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿å°ºå¯¸å¯¹é½ã€‚å¦‚æœä¸ä¸€æ ·å¤§ï¼Œé€šå¸¸ç”¨æ’å€¼ (Interpolate) å¯¹é½åˆ° Student çš„å°ºå¯¸
        if student_map.shape != teacher_map.shape:
             teacher_map = F.interpolate(teacher_map.unsqueeze(1), 
                                         size=student_map.shape[1:], 
                                         mode='bilinear').squeeze(1)

        loss_map = self.mse_loss(student_map, teacher_map)

        return loss_ce + self.alpha_map * loss_map
```

### 5\. ğŸ’¡ ä¸ºä»€ä¹ˆåŠ ä¸Šè¿™ä¸€æ­¥å¾ˆå…³é”®ï¼Ÿ

1.  **å¯è§£é‡Šæ€§ (Interpretability)**ï¼š
      * åŸæ¥çš„ Student åªæ˜¯ä¸ªé»‘ç›’ï¼Œè¾“å‡º0æˆ–1ã€‚
      * ç°åœ¨çš„ Student ä¼šè¾“å‡ºä¸€ä¸ª `interaction_map`ã€‚å¦‚æœé¢„æµ‹æ˜¯â€œç›¸äº’ä½œç”¨â€ï¼Œä½ å¯ä»¥ç”»å‡ºè¿™ä¸ªçƒ­åŠ›å›¾ï¼Œçœ‹åˆ° **åˆ°åº•æ˜¯å“ªä¸¤ä¸ªæ°¨åŸºé…¸ç‰‡æ®µåœ¨ç›¸äº’å¸å¼•**ã€‚
2.  **å¼ºå½’çº³åç½®**ï¼š
      * è›‹ç™½è´¨ç›¸äº’ä½œç”¨æœ¬è´¨ä¸Šå°±æ˜¯ç‰©ç†æ¥è§¦ã€‚
      * é€šè¿‡å¼ºè¿« CNN å­¦ä¹ è¿™ç§â€œæ¥è§¦çŸ©é˜µâ€ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨æ•™å®ƒå­¦ä¹ ç‰©ç†è§„åˆ™ï¼Œè€Œä¸ä»…ä»…æ˜¯ç»Ÿè®¡è§„å¾‹ã€‚
3.  **å¯¹ä»˜å°æ ·æœ¬çš„ç¥å™¨**ï¼š
      * ä»…é  1W æ¡æ•°æ®çš„ `0/1` æ ‡ç­¾å¾ˆéš¾å­¦åˆ°å¤æ‚çš„ç‰©ç†ç»“åˆä½ç‚¹ã€‚
      * ä½†æ˜¯é€šè¿‡è’¸é¦ ESM çš„ Attention Mapï¼ŒStudent ç›¸å½“äº**å…è´¹**çœ‹äº†å‡ äº¿ä¸ªè›‹ç™½è´¨å­¦åˆ°çš„ç»“æ„çŸ¥è¯†ã€‚è¿™æ¯”å•çº¯çš„ Logits è’¸é¦ï¼ˆLabel Smoothingï¼‰è•´å«çš„ä¿¡æ¯é‡å¤§å‡ ä¸ªæ•°é‡çº§ã€‚

### 6\. å®æ“ä¸­çš„ä¸€ä¸ªå‘ (Padding Mask)

åœ¨è®¡ç®— `interaction_map` æ—¶ï¼Œè¦æ³¨æ„ Padding çš„éƒ¨åˆ†ã€‚

  * `seq_a` å’Œ `seq_b` éƒ½æœ‰ Paddingï¼ˆè¡¥é›¶ï¼‰ã€‚
  * è®¡ç®—å‡ºçš„ Matrix è¾¹ç¼˜ä¼šæœ‰å¾ˆå¤šæ— æ•ˆçš„è®¡ç®—ã€‚
  * **è§£å†³åŠæ³•**ï¼šåœ¨è®¡ç®— Loss æ—¶ï¼Œä¹˜ä»¥ä¸€ä¸ª Maskï¼Œåªè®¡ç®—é Padding åŒºåŸŸçš„ MSE Lossã€‚

<!-- end list -->

```python
# ç®€å•çš„ Mask å¤„ç†
mask_a = (seq_a != 0).float().unsqueeze(2) # (B, La, 1)
mask_b = (seq_b != 0).float().unsqueeze(1) # (B, 1, Lb)
mask_matrix = torch.matmul(mask_a, mask_b) # (B, La, Lb)

loss_map = (loss_map * mask_matrix).sum() / mask_matrix.sum() # åªç®—æœ‰æ•ˆåŒºåŸŸ
```

è¿™ä¸ª **"Student-LoFTR" (Interaction Map Distillation)** åŠ ä¸Šä¹‹å‰çš„ **Semantic Distillation**ï¼Œå°±æ˜¯é’ˆå¯¹å°æ ·æœ¬ PPI ä»»åŠ¡æœ€æè‡´çš„è§£å†³æ–¹æ¡ˆäº†ã€‚