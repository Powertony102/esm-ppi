è¿™æ˜¯ä¸€ä¸ªéå¸¸æ·±åˆ»ä¸”åˆ‡ä¸­ç—›ç‚¹çš„æ´å¯Ÿã€‚åœ¨å°æ ·æœ¬ï¼ˆ\~1Wæ•°æ®ï¼‰ä¸‹å¾®è°ƒäº¿çº§å‚æ•°çš„ESM-2ç¡®å®å®¹æ˜“é™·å…¥è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰æˆ–è€…ç¾éš¾æ€§é—å¿˜ï¼Œè€Œä¸”ä¼˜åŒ–æå…¶å›°éš¾ã€‚

é‡‡ç”¨ **çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillation, KDï¼‰** æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„ç­–ç•¥ã€‚

### ğŸ’¡ æ–°æ–¹æ¡ˆæ ¸å¿ƒæ€æƒ³ï¼šTeacher-Student æ¶æ„

  * **Teacher (æ•™å¸ˆæ¨¡å‹)**: **ESM-2 (å†»ç»“å‚æ•°)**ã€‚åˆ©ç”¨å…¶åœ¨å¤§è§„æ¨¡è›‹ç™½è´¨åº“ä¸Šå­¦åˆ°çš„é€šç”¨ç‰¹å¾ï¼Œæä¾›â€œè½¯æ ‡ç­¾ï¼ˆSoft Labelsï¼‰â€æˆ–â€œç‰¹å¾å¼•å¯¼â€ã€‚å®ƒä¸ä»…å‘Šè¯‰å­¦ç”Ÿâ€œæ˜¯/å¦â€ï¼Œè¿˜å‘Šè¯‰å­¦ç”Ÿâ€œæœ‰å¤šåƒâ€ã€‚
  * **Student (å­¦ç”Ÿæ¨¡å‹)**: **è½»é‡çº§ 1D-CNN (ResNet-1D)**ã€‚å·ç§¯ç¥ç»ç½‘ç»œå½’çº³åç½®å¼ºï¼Œå‚æ•°å°‘ï¼Œæ›´é€‚åˆå°æ ·æœ¬æ•°æ®ï¼Œä¸”æ¨ç†é€Ÿåº¦æå¿«ã€‚
  * **ç›®æ ‡**: è®©è½»é‡çº§çš„CNNå»æ¨¡ä»¿ESM-2çš„é¢„æµ‹åˆ†å¸ƒï¼ŒåŒæ—¶ç»“åˆçœŸå®æ ‡ç­¾ï¼ˆGround Truthï¼‰è¿›è¡Œç›‘ç£å­¦ä¹ ã€‚

-----

### 1\. ğŸ—ï¸ æ–°æ¨¡å‹æ¶æ„å›¾

```mermaid
graph TD
    subgraph Data
    Seq[åºåˆ—è¾“å…¥ Sequence]
    end

    subgraph Teacher_Model [Teacher: Frozen ESM-2]
    ESM[ESM-2 Encoder]
    ET[Teacher Logits]
    ESM --> |å†»ç»“å‚æ•°| ET
    end

    subgraph Student_Model [Student: Siamese ResNet-CNN]
    Emb[Learnable Embedding]
    CNN1[1D-CNN Block 1]
    CNN2[1D-CNN Block 2]
    Pool[Global Max Pooling]
    FC[Classifier Head]
    SL[Student Logits]
    
    Seq --> Emb --> CNN1 --> CNN2 --> Pool --> FC --> SL
    end

    Seq --> ESM
    
    subgraph Loss_Function [Distillation Loss]
    CE[Hard Loss <br> CrossEntropy vs True Label]
    KL[Soft Loss <br> KL-Div vs Teacher Logits]
    Total[Total Loss = Î±*CE + (1-Î±)*KL]
    
    SL --> CE
    SL --> KL
    ET --> KL
    end
```

-----

### 2\. ğŸ’» æ ¸å¿ƒä»£ç å®ç° (PyTorch)

æˆ‘ä»¬éœ€è¦ä¸‰ä¸ªéƒ¨åˆ†ï¼š

1.  **Studentæ¨¡å‹**ï¼šä¸€ä¸ªé€‚åˆåºåˆ—æ•°æ®çš„å­ªç”Ÿå·ç§¯ç½‘ç»œã€‚
2.  **è’¸é¦Loss**ï¼šç»“åˆåˆ†ç±»Losså’Œè’¸é¦Lossã€‚
3.  **è®­ç»ƒå¾ªç¯**ï¼šåŒæ—¶è¿è¡ŒTeacherå’ŒStudentã€‚

#### A. å®šä¹‰ Student æ¨¡å‹ (Siamese 1D-CNN)

è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„TextCNN/ResNetå˜ä½“ï¼Œä¸“é—¨å¤„ç†åºåˆ—ï¼Œå‚æ•°é‡æå°ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock1D(nn.Module):
    """ä¸€ç»´æ®‹å·®å·ç§¯å—ï¼Œç”¨äºæå–åºåˆ—å±€éƒ¨ç‰¹å¾"""
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super().__init__()
        padding = (kernel_size - 1) * dilation // 2
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        # å¦‚æœé€šé“æ•°æ”¹å˜ï¼Œéœ€è¦é€šè¿‡1x1å·ç§¯è°ƒæ•´æ®‹å·®è¿æ¥
        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        residual = self.shortcut(x)
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += residual
        out = self.relu(out)
        return out

class StudentPPI_CNN(nn.Module):
    def __init__(self, vocab_size=25, embed_dim=64, hidden_dim=128):
        super().__init__()
        # 1. Embeddingå±‚ï¼šå­¦ä¹ æ°¨åŸºé…¸çš„ä½ç»´è¡¨ç¤º
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # 2. Backboneï¼šç‰¹å¾æå– (ç±»ä¼¼äºResNetçš„ç»“æ„)
        # å°† Sequence (Batch, Len, Dim) è½¬ç½®ä¸º (Batch, Dim, Len) ä¾›Conv1dä½¿ç”¨
        self.encoder = nn.Sequential(
            ResidualBlock1D(embed_dim, hidden_dim, kernel_size=3),
            nn.MaxPool1d(2),
            ResidualBlock1D(hidden_dim, hidden_dim*2, kernel_size=5),
            nn.AdaptiveMaxPool1d(1) # å…¨å±€æ± åŒ–ï¼Œæ— è®ºåºåˆ—å¤šé•¿éƒ½å˜æˆå›ºå®šå‘é‡
        )
        
        # 3. åˆ†ç±»å¤´
        # ä¸¤ä¸ªè›‹ç™½è´¨çš„ç‰¹å¾æ‹¼æ¥ï¼š(hidden_dim*2) * 2
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2 * 2, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2) # è¾“å‡ºLogits (æœªç»è¿‡Softmax)
        )

    def forward_one_branch(self, x):
        # x: (Batch, Seq_Len)
        x = self.embedding(x)           # (Batch, Seq_Len, Embed_Dim)
        x = x.permute(0, 2, 1)          # (Batch, Embed_Dim, Seq_Len)
        feat = self.encoder(x)          # (Batch, Hidden_Dim*2, 1)
        return feat.squeeze(-1)         # (Batch, Hidden_Dim*2)

    def forward(self, seq_a, seq_b):
        # å­ªç”Ÿç½‘ç»œç»“æ„ï¼šå…±äº«æƒé‡
        feat_a = self.forward_one_branch(seq_a)
        feat_b = self.forward_one_branch(seq_b)
        
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([feat_a, feat_b], dim=1)
        logits = self.classifier(combined)
        return logits
```

#### B. è’¸é¦æŸå¤±å‡½æ•° (Distillation Loss)

è¿™æ˜¯è’¸é¦çš„æ ¸å¿ƒã€‚æˆ‘ä»¬éœ€è¦è®¡ç®— KL æ•£åº¦ã€‚

```python
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.5, temperature=4.0):
        super().__init__()
        self.alpha = alpha  # è’¸é¦lossçš„æƒé‡
        self.T = temperature # æ¸©åº¦ç³»æ•°ï¼šè¶Šé«˜ï¼ŒSoftmaxåˆ†å¸ƒè¶Šå¹³æ»‘ï¼ŒåŒ…å«è¶Šå¤š"æš—çŸ¥è¯†"
        self.hard_loss = nn.CrossEntropyLoss()
        self.soft_loss = nn.KLDivLoss(reduction="batchmean")

    def forward(self, student_logits, teacher_logits, labels):
        # 1. Hard Loss: å­¦ç”Ÿå¿…é¡»é¢„æµ‹å¯¹çœŸå®æ ‡ç­¾
        loss_hard = self.hard_loss(student_logits, labels)
        
        # 2. Soft Loss: å­¦ç”Ÿçš„Softmaxåˆ†å¸ƒè¦åƒè€å¸ˆ
        # æ³¨æ„ï¼šKLDivLossè¦æ±‚è¾“å…¥æ˜¯log_softmaxï¼Œç›®æ ‡æ˜¯softmax
        student_soft = F.log_softmax(student_logits / self.T, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.T, dim=1)
        
        loss_soft = self.soft_loss(student_soft, teacher_soft) * (self.T ** 2)
        
        # 3. ç»„åˆ
        total_loss = (1 - self.alpha) * loss_hard + self.alpha * loss_soft
        return total_loss
```

#### C. è®­ç»ƒå¾ªç¯é€»è¾‘

**æ³¨æ„**ï¼šä¸ºäº†èŠ‚çœæ˜¾å­˜ï¼Œé€šå¸¸ä¸éœ€è¦åœ¨è®­ç»ƒæ—¶å®æ—¶è·‘ESMã€‚æœ€ä½³å®è·µæ˜¯**é¢„å…ˆè®¡ç®—**å¥½æ‰€æœ‰è®­ç»ƒæ•°æ®çš„ESM Logitsï¼ˆæˆ–è€…Embeddingsï¼‰ï¼Œä¿å­˜ä¸‹æ¥ï¼Œè®­ç»ƒStudentæ—¶ç›´æ¥åŠ è½½ã€‚

ä½†ä¸ºäº†æ¼”ç¤ºå®Œæ•´æµç¨‹ï¼Œè¿™é‡Œå†™å‡ºå®æ—¶è’¸é¦çš„é€»è¾‘ï¼š

```python
# å‡è®¾ä½ å·²ç»æœ‰äº† dataloader
# teacher_model: åŠ è½½å¥½çš„ ESM-2 (ä¾‹å¦‚ esm2_t33_650M_UR50D)
# student_model: ä¸Šé¢å®šä¹‰çš„ StudentPPI_CNN

def train_distillation_step(dataloader, teacher_model, student_model, optimizer, device):
    criterion = DistillationLoss(alpha=0.5, temperature=4.0)
    teacher_model.eval() # è€å¸ˆæ°¸è¿œæ˜¯evalæ¨¡å¼
    student_model.train()
    
    total_loss = 0
    
    for batch in dataloader:
        # æ•°æ®å‡†å¤‡
        # æ³¨æ„ï¼šStudentå’ŒTeacherå¯èƒ½éœ€è¦ä¸åŒçš„Tokenizer
        # Studentè¾“å…¥: ç®€å•çš„æ•´æ•°ç´¢å¼• (0-20)
        # Teacherè¾“å…¥: ESM Tokenizerçš„è¾“å‡º
        seq_a_student, seq_b_student = batch['student_input'] 
        seq_a_teacher, seq_b_teacher = batch['teacher_input']
        labels = batch['labels'].to(device)
        
        seq_a_student, seq_b_student = seq_a_student.to(device), seq_b_student.to(device)
        seq_a_teacher, seq_b_teacher = seq_a_teacher.to(device), seq_b_teacher.to(device)

        # 1. è·å– Teacher Logits (ä¸è®¡ç®—æ¢¯åº¦)
        with torch.no_grad():
            # è¿™é‡Œå‡è®¾ä½ æœ‰ä¸ªå‡½æ•°æŠŠESMè¾“å‡ºè½¬ä¸ºåˆ†ç±»Logitsï¼Œæˆ–è€…ç›´æ¥ç”¨ESMæå–ç‰¹å¾åæ¥ä¸ªå†»ç»“çš„åˆ†ç±»å¤´
            # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œå‡è®¾ teacher_output ç›´æ¥ç»™å‡ºäº† (Batch, 2) çš„ logits
            # å®é™…æ“ä½œä¸­ï¼Œé€šå¸¸æ˜¯: embedding = esm(seq); logits = frozen_classifier(embedding)
            teacher_logits = teacher_model(seq_a_teacher, seq_b_teacher) 

        # 2. è·å– Student Logits
        student_logits = student_model(seq_a_student, seq_b_student)
        
        # 3. è®¡ç®—è’¸é¦ Loss
        loss = criterion(student_logits, teacher_logits, labels)
        
        # 4. åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
    return total_loss / len(dataloader)
```

### 3\. ğŸŒŸ ä¸ºä»€ä¹ˆè¿™ä¸ªæ–¹æ¡ˆæ›´å¥½ï¼Ÿ

1.  **æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º**ï¼šStudentæ¨¡å‹ï¼ˆCNNï¼‰å‚æ•°æå°‘ï¼ˆå¯èƒ½åªæœ‰å‡ ä¸‡åˆ°å‡ åä¸‡å‚æ•°ï¼‰ï¼Œç›¸æ¯”ESMçš„å‡ äº¿å‚æ•°ï¼Œåœ¨1Wæ¡æ•°æ®ä¸Šå¾ˆéš¾è¿‡æ‹Ÿåˆã€‚
2.  **è·å–äº†é«˜çº§è¯­ä¹‰**ï¼šé€šè¿‡KL Lossï¼ŒStudentä¸ä»…ä»…å­¦åˆ°äº†â€œ0/1â€ï¼Œè¿˜å­¦åˆ°äº†â€œè¿™ä¸ªæ ·æœ¬è™½ç„¶æ˜¯0ï¼Œä½†æœ‰ç‚¹åƒ1â€ï¼Œè¿™ç§å¹³æ»‘çš„æ ‡ç­¾å¤§å¤§é™ä½äº†å­¦ä¹ éš¾åº¦ã€‚
3.  **æ¨ç†æé€Ÿ**ï¼šæ¨¡å‹ä¸Šçº¿åï¼Œä½ åªéœ€è¦éƒ¨ç½²Studentæ¨¡å‹ã€‚CNNçš„æ¨ç†é€Ÿåº¦æ¯”Transformerå¿«å‡ åå€ã€‚

### 4\. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1.  **ç¦»çº¿ç”Ÿæˆ Teacher Logits**ï¼š
      * ä¸è¦åœ¨è®­ç»ƒå¾ªç¯é‡Œè·‘ESMï¼ˆå¤ªæ…¢æ˜¾å­˜ä¸å¤Ÿï¼‰ã€‚
      * å†™ä¸€ä¸ªè„šæœ¬ï¼Œç”¨ESM-2ï¼ˆ650Mæˆ–3Bç‰ˆæœ¬ï¼‰è·‘ä¸€é`train.csv`ï¼ŒæŠŠæ¯å¯¹è›‹ç™½è´¨çš„é¢„æµ‹logitsä¿å­˜æˆnumpyæ–‡ä»¶ã€‚
      * è®­ç»ƒStudentæ—¶ï¼Œç›´æ¥åŠ è½½è¿™äº›logitsä½œä¸ºtargetã€‚
2.  **Student Tokenizer**ï¼š
      * Studentä¸éœ€è¦ESMå¤æ‚çš„Tokenizerã€‚åªéœ€å»ºç«‹ç®€å•çš„æ˜ å°„ï¼š`{'A':1, 'C':2, ...}` å³å¯ã€‚
3.  **æ•°æ®å¢å¼º**ï¼š
      * ç”±äºæ˜¯CNNï¼Œä½ å¯ä»¥å¯¹åºåˆ—åšç®€å•çš„å¢å¼ºï¼ˆå¦‚éšæœºMaskæ‰å‡ ä¸ªæ°¨åŸºé…¸ï¼‰ï¼Œè¿›ä¸€æ­¥å¢åŠ é²æ£’æ€§ã€‚

è¿™ä¸ªæ–¹æ¡ˆæ˜¯å·¥ä¸šç•Œå¤„ç†â€œå°æ ·æœ¬+å¤§æ¨¡å‹â€æœ€æ ‡å‡†çš„è½åœ°è§£æ³•ã€‚